# -*- coding: utf-8 -*-
"""Untitled12.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1f2Av83jAsAMlmkjRwTeVqc3TqmBIm06z
"""

# Commented out IPython magic to ensure Python compatibility.
from scipy.stats import mode
import numpy as np
#from mnist import MNIST
from time import time
import pandas as pd
import os
import matplotlib.pyplot as matplot
import matplotlib
# %matplotlib inline

import random
matplot.rcdefaults()
from IPython.display import display, HTML
from itertools import chain
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import seaborn as sb
from sklearn.model_selection import ParameterGrid
from sklearn.svm import SVC, LinearSVC
import warnings
import tensorflow_datasets as tfds
import tensorflow as tf
warnings.filterwarnings('ignore')

mnist = tf.keras.datasets.mnist

from keras.utils import to_categorical

import zipfile,cv2

local_zip = '/content/USPS_images.zip'
zip_ref = zipfile.ZipFile(local_zip, 'r')
zip_ref.extractall('/content/trainntest')

train_dir = '/content/trainntest/train'
validation_dir = '/content/trainntest/test'
y_train = []
y_test = []
for path in os.listdir(train_dir):
    if os.path.isfile(os.path.join(train_dir, path)):
        y_train.append(int(path[0]))

for path in os.listdir(validation_dir):
  if os.path.isfile(os.path.join(validation_dir, path)):
      y_test.append(int(path[0]))

x_train = []
for path in os.listdir(train_dir):
  if os.path.isfile(os.path.join(train_dir,path)):
    x_train.append(cv2.cvtColor(cv2.imread(f"{train_dir}/{path}"), cv2.COLOR_RGB2GRAY))

x_test = []
for path in os.listdir(validation_dir):
  if os.path.isfile(os.path.join(validation_dir,path)):
    x_test.append(cv2.cvtColor(cv2.imread(f"{validation_dir}/{path}"), cv2.COLOR_RGB2GRAY))

x_train = np.array(x_train)
x_test = np.array(x_test)
y_train = np.array(y_train)
y_test = np.array(y_test)


x_train_final = x_train.reshape(-1 ,16*16) / 255
x_test_final = x_test.reshape(-1 ,16*16) / 255

train, trlab, test, tslab  = x_train_final, y_train, x_test_final, y_test

train.shape

def pick(train, trlab, test, tslab, percentage):
  train = train[:int(len(train)*percentage)]
  test = test[:int(len(test)*percentage)]
  trlab = trlab[:int(len(trlab)*percentage)]
  tslab = tslab[:int(len(tslab)*percentage)]
  return train, trlab, test, tslab

train, trlab, test, tslab = pick(train, trlab, test, tslab, 0.05)

svm = LinearSVC(dual=False, verbose=1)
svm.fit(train, trlab)

svm.coef_
svm.intercept_

pred = svm.predict(test)

accuracy_score(tslab, pred) # Accuracy

cm = confusion_matrix(tslab, pred)
matplot.subplots(figsize=(10, 6))
sb.heatmap(cm, annot = True, fmt = 'g')
matplot.xlabel("Predicted")
matplot.ylabel("Actual")
matplot.title("Confusion Matrix")
matplot.show()

"""**with Penalty(L2)**
power 2
"""

acc = []
acc_tr = []
coefficient = []
for c in [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]:
    svm = LinearSVC(dual=False, C=c)
    svm.fit(train, trlab)
    coef = svm.coef_
    
    p_tr = svm.predict(train)
    a_tr = accuracy_score(trlab, p_tr)
    
    pred = svm.predict(test)
    a = accuracy_score(tslab, pred)
    
    coefficient.append(coef)
    acc_tr.append(a_tr)
    acc.append(a)
    print(f"done for {c}")

c = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]

matplot.subplots(figsize=(10, 5))
matplot.semilogx(c, acc,'-gD' ,color='red' , label="Testing Accuracy")
matplot.semilogx(c, acc_tr,'-gD' , label="Training Accuracy")
#matplot.xticks(L,L)
matplot.grid(True)
matplot.xlabel("Cost Parameter C")
matplot.ylabel("Accuracy")
matplot.legend()
matplot.title('Accuracy versus the Cost Parameter C (log-scale)')
matplot.show()

"""**c = 1 : the Best**"""

svm_coef = coefficient[4]
svm_coef.shape

matplot.subplots(2,5, figsize=(24,10))
for i in range(10):
    l1 = matplot.subplot(2, 5, i + 1)
    l1.imshow(svm_coef[i].reshape(16, 16), cmap=matplot.cm.RdBu)
    l1.set_xticks(())
    l1.set_yticks(())
    l1.set_xlabel('Class %i' % i)
matplot.suptitle('Class Coefficients')
matplot.show()

"""**With Penalty (L1)**"""

acc = []
acc_tr = []
coefficient = []
for c in [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]:
    svm = LinearSVC(dual=False, C=c, penalty='l1')
    svm.fit(train, trlab)
    coef = svm.coef_
    
    p_tr = svm.predict(train)
    a_tr = accuracy_score(trlab, p_tr)
    
    pred = svm.predict(test)
    a = accuracy_score(tslab, pred)
    
    coefficient.append(coef)
    acc_tr.append(a_tr)
    acc.append(a)

c = [0.0001,0.001,0.01,0.1,1,10,100,1000,10000]

matplot.subplots(figsize=(10, 5))
matplot.semilogx(c, acc,'-gD' ,color='red' , label="Testing Accuracy")
matplot.semilogx(c, acc_tr,'-gD' , label="Training Accuracy")
#matplot.xticks(L,L)
matplot.grid(True)
matplot.xlabel("Cost Parameter C")
matplot.ylabel("Accuracy")
matplot.legend()
matplot.title('Accuracy versus the Cost Parameter C (log-scale)')
matplot.show()

"""**c = 1 : the Best**"""

svm_coef = coefficient[4]
svm_coef.shape

matplot.subplots(2,5, figsize=(24,10))
for i in range(10):
    l1 = matplot.subplot(2, 5, i + 1)
    l1.imshow(svm_coef[i].reshape(16, 16), cmap=matplot.cm.RdBu)
    l1.set_xticks(())
    l1.set_yticks(())
    l1.set_xlabel('Class %i' % i)
matplot.suptitle('Class Coefficients')
matplot.show()

"""**SVM RBF Kernel**

generate a random sample of the data and check how the distribution is compared to the original distribution
"""

seq = np.random.randint(0,len(train),int(0.6*(len(train))))
train_samp = train[seq]
trlab_samp = trlab[seq]

train_samp.shape
trlab_samp.shape

seq = np.random.randint(0,len(test),int(0.6*(len(test))))
test_samp = test[seq]
tslab_samp = tslab[seq]

test_samp.shape
tslab_samp.shape

fig, ax = matplot.subplots(1,2, figsize=(10,4))
ax[0].hist(trlab_samp)
ax[1].hist(trlab)
fig.show
matplot.show()

"""**Running SVC for multiple cost factor(s) C and Gamma**"""

coefficient = []
n_supp = []
sup_vec = []
i = 0
df = pd.DataFrame(columns = ['c','gamma','train_acc','test_acc'])
for c in [0.01, 0.1, 1, 10, 100]:
    for g in [0.01, 0.1, 1, 10, 100]:
        svm = SVC(kernel='rbf', C=c, gamma=g)
        model = svm.fit(train_samp, trlab_samp)
        globals()['model%s' % i] = model
        d_coef = svm.dual_coef_
        support = svm.n_support_
        sv = svm.support_
    
        p_tr = svm.predict(train_samp)
        a_tr = accuracy_score(trlab_samp, p_tr)
    
        pred = svm.predict(test_samp)
        a = accuracy_score(tslab_samp, pred)
    
        coefficient.append(d_coef)
        n_supp.append(support)
        sup_vec.append(sv)
        df.loc[i] = [c,g,a_tr,a]
        i=i+1

df

"""We choose C=10 and Gamma=0.01 to look at the Support vectors"""

pd.DataFrame(coefficient[15]) # dual_coef_

pd.DataFrame(n_supp[15]) # n_support_

"""**Sampling one positive support vector for each class**"""

ind = 0
matplot.subplots(2,5, figsize=(24,10))
for i in range(len(n_supp[15])):
    l1 = matplot.subplot(2, 5, i + 1)
    sv_image = train_samp[sup_vec[15][ind:ind+n_supp[15][i]]][0]
    l1.imshow(sv_image.reshape(16, 16), cmap=matplot.cm.RdBu)
    l1.set_xticks(())
    l1.set_yticks(())
    l1.set_xlabel('Class %i vs All' % i)
    ind = ind + n_supp[15][i]
matplot.suptitle('Support Vectors for Positive Classes')
matplot.show()

"""**Sampling one negative support vector for each class**"""

ind = n_supp[6][0]
matplot.subplots(2,5, figsize=(24,10))
for i in range(len(n_supp[6])-1):
    l1 = matplot.subplot(2, 5, i + 1)
    sv_image = train_samp[sup_vec[6][ind:ind+n_supp[6][i+1]]][13]
    l1.imshow(sv_image.reshape(16, 16), cmap=matplot.cm.RdBu)
    l1.set_xticks(())
    l1.set_yticks(())
    l1.set_xlabel('Class %i vs All' % i)
    ind = ind + n_supp[6][i+1]
ind = 0
l1 = matplot.subplot(2, 5, 10)
sv_image = train_samp[sup_vec[6][ind:ind+n_supp[6][0]]][13]
l1.imshow(sv_image.reshape(16, 16), cmap=matplot.cm.RdBu)
l1.set_xticks(())
l1.set_yticks(())
l1.set_xlabel('Class 9 vs All')
matplot.suptitle('Support Vectors for Negative Classes')
matplot.show()

"""**SVC Poly kernel**"""

seq = np.random.randint(0,len(train),int(0.6*(len(train))))
train_samp = train[seq]
trlab_samp = trlab[seq]

train_samp.shape
trlab_samp.shape

seq = np.random.randint(0,len(test),int(0.6*(len(test))))
test_samp = test[seq]
tslab_samp = tslab[seq]

test_samp.shape
tslab_samp.shape

"""**Running SVC for multiple cost factor(s) C and Degree**"""

coefficient = []
n_supp = []
sup_vec = []
i = 0
df = pd.DataFrame(columns = ['c','degree','train_acc','test_acc'])
for c in [0.01, 0.1, 1, 10, 100]:
    for d in [2,3,4,5,6]:
        svm = SVC(kernel='poly', C=c, degree=d)
        model = svm.fit(train_samp, trlab_samp)
        globals()['model%s' % i] = model
        d_coef = svm.dual_coef_
        support = svm.n_support_
        sv = svm.support_
    
        p_tr = svm.predict(train_samp)
        a_tr = accuracy_score(trlab_samp, p_tr)
    
        pred = svm.predict(test_samp)
        a = accuracy_score(tslab_samp, pred)
    
        coefficient.append(d_coef)
        n_supp.append(support)
        sup_vec.append(sv)
        df.loc[i] = [c,d,a_tr,a]
        i=i+1

df

"""**We choose C=100 and Degree=2 to look at the Support vectors**"""

pd.DataFrame(coefficient[20]) # dual_coef_

pd.DataFrame(n_supp[20]) # n_support_

"""**Sampling one positive support vector for each class**"""

ind = 0
matplot.subplots(2,5, figsize=(24,10))
for i in range(len(n_supp[20])):
    l1 = matplot.subplot(2, 5, i + 1)
    sv_image = train_samp[sup_vec[20][ind:ind+n_supp[20][i]]][0]
    l1.imshow(sv_image.reshape(16, 16), cmap=matplot.cm.RdBu)
    l1.set_xticks(())
    l1.set_yticks(())
    l1.set_xlabel('Class %i vs All' % i)
    ind = ind + n_supp[20][i]
matplot.suptitle('Support Vectors for Positive Classes')
matplot.show()

"""**Sampling one negative support vector for each class**"""

ind = n_supp[6][0]
matplot.subplots(2,5, figsize=(24,10))
for i in range(len(n_supp[6])-1):
    l1 = matplot.subplot(2, 5, i + 1)
    sv_image = train_samp[sup_vec[6][ind:ind+n_supp[6][i+1]]][13]
    l1.imshow(sv_image.reshape(16, 16), cmap=matplot.cm.RdBu)
    l1.set_xticks(())
    l1.set_yticks(())
    l1.set_xlabel('Class %i vs All' % i)
    ind = ind + n_supp[6][i+1]
ind = 0
l1 = matplot.subplot(2, 5, 10)
sv_image = train_samp[sup_vec[6][ind:ind+n_supp[6][0]]][13]
l1.imshow(sv_image.reshape(16, 16), cmap=matplot.cm.RdBu)
l1.set_xticks(())
l1.set_yticks(())
l1.set_xlabel('Class 9 vs All')
matplot.suptitle('Support Vectors for Negative Classes')
matplot.show()